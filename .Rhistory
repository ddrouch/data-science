phd.reg
#More information for hypothesis tests
summary(phd.reg)
#confidence intervals for parameter estimates
confint(phd.reg)
install.packages(c("plot3D", "ppcor", "QuantPsyc"))
phd <- read.table(file.choose(), header=TRUE, sep="\t")
View(phd)
require(plot3D)
attach(phd)
require(plot3D)
attach(phd)
fit <- lm(PUBS ~ TIME+SALARY)
TIME.pred <- seq(0, 20, length.out = 30)
SALARY.pred <- seq(40000, 80000, length.out = 30)
xy <- expand.grid(TIME = TIME.pred,
SALARY = SALARY.pred)
PUBS.pred <- matrix (nrow = 30, ncol = 30,
data = predict(fit, newdata = data.frame(xy), interval = "prediction"))
#Recall in async we mean centered salary, let's do that first
#(You may want to review the week 1 R code on how to add a variable to an existing dataset)
#dataset$NEWVARNAME = DATASET$VARWEWANTOCENTER - mean(DATASET$VARWEWANTTOCENTER)
phd$SALARY_c <- phd$SALARY - mean(phd$SALARY)
#Recall in async we mean centered salary, let's do that first
#(You may want to review the week 1 R code on how to add a variable to an existing dataset)
#dataset$NEWVARNAME = DATASET$VARWEWANTOCENTER - mean(DATASET$VARWEWANTTOCENTER)
phd$SALARY_c <- phd$SALARY - mean(phd$SALARY)
phd$SALARY_c <- phd$SALARY - mean(phd$SALARY)
phd$CITS_c <- phd$CITS - mean(phd$CITS)
phd.reg <- lm(PUBS ~ TIME + SALARY_c + CITS_c, data=phd)
summary(phd.reg)
phd.reg <- lm(PUBS ~ TIME + SALARY_c + CITS, data=phd)
summary(phd.reg)
install.packages("QuantPsyc")
library(QuantPsyc)
lm.beta(phd.reg)
install.packages("ppcor")
library(ppcor)
library(ppcor)
library(ppcor)
spcor(phd[,c("PUBS","SALARY_c","TIME","CITS")])
bloodpressure = read.table("/Users/derekrouch/Google Drive/00 Vanderbilt/LLO 8180 - Applied Statistics/Week 13/Bloodpressure.txt", header=T, sep="\t")
View(bloodpressure)
bloodpressure$Age_c <- bloodpressure$Age - mean(bloodpressure$Age)
bloodpressure$Size_c <- bloodpressure$Size - mean(bloodpressure$Size)
bloodpressure.reg = lm(SBP ~ Size_c + Age_c, data=bloodpressure)
summary(bloodpressure.reg)
install.packages("QuantPsyc")
library(QuantPsyc)
lm.beta(bloodpressure.reg)
mean(bloodpressure$Age)
mean(bloodpressure$Size)
51-52.68
3.3-3.41796
install.packages("ppcor")
library(ppcor)
library(ppcor)
spcor(bloodpressue[,c("SBP","SALARY_c","TIME","CITS")])
spcor(bloodpressue[,c("SBP","Size_c","Age_c")])
spcor(bloodpressure[,c("SBP","Size_c","Age_c")])
0.2459132^2
0.2636291^2
spcor(bloodpressure[,c("SBP","Age_c")])
pcor(bloodpressure[,c("SBP","Age_c")])
pcor(bloodpressure[,c("SBP","Size_c","Age_c")])
0.3435182^2
tutoring = read.table("/Users/derekrouch/Downloads/tutoring_and_gpa.txt", header=T, sep="\t")
View(tutoring)
View(tutoring)
View(tutoring)
View(tutoring)
tutoring_gpa = read.table("/Users/derekrouch/Downloads/tutoring_and_gpa.txt", header=T, sep=",")
View(tutoring_gpa)
tutoring_gpa$age_c = tutoring_gpa$age - mean(tutoring_gpa$age)
tutoring_gpa.reg = lm(gpa ~ num_tut_hrs + age_c, data=tutoring_gpa)
summary(tutoring_gpa.reg)
mean(tutoring_gpa$age)
mean(bloodpressure$Age)
View(bloodpressure)
mean(bloodpressure$Size)
format <- c(17,54,64)
chisq.test(format, p=c(1/3,1/3,1/3))
graduate <- read.table(file.choose(), header=TRUE, sep="\t")
View(graduate)
gradtable <- table(graduate$Business,graduate$Doctorate)
gradtable
View(gradtable)
gradetable
gradtable
chisq.test(gradtable,correct=F)
sesstem <- read.table(file.choose(), header=TRUE, sep=",")
install.packages("Rcmdr")
library(tidyverse)
## Where are we?
sc%>%filter(instnm=="Vanderbilt University")
library(tidyverse)
## Where are we?
sc%>%filter(instnm=="Vanderbilt University")
## Load in the data
load("college.Rdata")
View(sc)
#library(gdata)  #gdata is finicky ... you may need to install it, but not load it.
library(tidyverse)
library(haven)
library(readxl)
hsb<-read_csv(file="/Users/derekrouch/Documents/GitHub/data-science/hsb.csv")
View(hsb)
https://stats.idre.ucla.edu/wp-content/uploads/2016/02/hsb2-2.csv
hsb<-read_csv(file="https:/stats.idre.ucla.edu/wp-content/uploads/2016/02/hsb2-2.csv")
hsb<-read_csv(file="https://stats.idre.ucla.edu/wp-content/uploads/2016/02/hsb2-2.csv")
hsb<-read_csv(file="https://stats.idre.ucla.edu/wp-content/uploads/2016/02/hsb2-2.csv")
hsb<-read_csv(file="/Users/derekrouch/Documents/GitHub/data-science/hsb.csv")
write_csv(hsb,path="hsb.csv")
hsb%>%mutate(ses=as.character(ses))->hsb
hsb%>%mutate(ses=as.numeric(ses))->hsb
#Check it out
head(hsb)
##Need these for later
my.names<-names(hsb)
#Write this in a variety of formats to be used later
write_delim(hsb, path="hsb.txt",delim="\t")
write_delim(hsb, path="hsb_semicolon.txt",delim=";")
gdata::write.fwf(data.frame(hsb),file="hsb.dat",sep="",colnames=FALSE)
install.packages(gdata)
install.packages("gdata")
library(gdata)
my.widths=c(3,#id
1, #female
1, #race
1, #ses
1, #schtyp
1, #prog
2, #read
2, #write
2, #math
2, #science
2 #socst
)
my_positions<-fwf_widths(my.widths)
View(my_positions)
my_positions<-fwf_widths(my.widths)
View(my_positions)
hsb3<-read_fwf("hsb.dat",
col_positions=my_positions)
head(hsb3)
names(hsb3)<-my.names
View(hsb3)
head(hsb3)
hsb_stata<-read_dta("https://stats.idre.ucla.edu/stat/stata/notes/hsb2.dta")
head(hsb_stata)
#SPSS
example_spss<-read_spss("https://stats.idre.ucla.edu/stat/data/binary.sav")
head(example_spss)
#SAS
hsb_sas<-read_sas("https://stats.idre.ucla.edu/wp-content/uploads/2016/02/hsb2.sas7bdat")
head(hsb_sas)
if(file.exists("free.xls")==FALSE){
download.file("http://nces.ed.gov/programs/digest/d14/tables/xls/tabn204.10.xls",destfile="free.xls")
free<-read_excel("free.xls",skip=4,col_names=FALSE)
}else{
free<-read_excel("free.xls",skip=4,col_names=FALSE)
}
if(file.exists("free.xls")==FALSE){
download.file("http://nces.ed.gov/programs/digest/d14/tables/xls/tabn204.10.xls",destfile="free.xls")
free<-read_excel("free.xls",skip=4,col_names=FALSE)
}else{
free<-read_excel("free.xls",skip=4,col_names=FALSE)
}
free<-read_excel("free.xls",skip=4,col_names=FALSE)
free<-read_excel("/Users/derekrouch/Documents/GitHub/data-science/free.xls",skip=4,col_names=FALSE)
library(readxl)
free<-read_excel("/Users/derekrouch/Documents/GitHub/data-science/free.xls",skip=4,col_names=FALSE)
free<-read_xls("/Users/derekrouch/Documents/GitHub/data-science/free.xls",skip=4,col_names=FALSE)
if(file.exists("/Users/derekrouch/Documents/GitHub/data-science/free.xls")==FALSE){
download.file("http://nces.ed.gov/programs/digest/d14/tables/xls/tabn204.10.xls",destfile="free.xls")
free<-read_excel("/Users/derekrouch/Documents/GitHub/data-science/free.xls",skip=4,col_names=FALSE)
}else{
free<-read_excel("/Users/derekrouch/Documents/GitHub/data-science/free.xls",skip=4,col_names=FALSE)
}
read.xls("/Users/derekrouch/Documents/GitHub/data-science/free.xls",skip=4, col_names=FALSE)
#library(gdata)  #gdata is finicky ... you may need to install it, but not load it.
library(tidyverse)
library(haven)
library(readxl)
## Web page:
##http://nces.ed.gov/programs/digest/d14/tables/dt14_204.10.asp
if(file.exists("/Users/derekrouch/Documents/GitHub/data-science/free.xls")==FALSE){
download.file("http://nces.ed.gov/programs/digest/d14/tables/xls/tabn204.10.xls",destfile="free.xls")
free<-read_excel("/Users/derekrouch/Documents/GitHub/data-science/free.xls",skip=4,col_names=FALSE)
}else{
free<-read_excel("/Users/derekrouch/Documents/GitHub/data-science/free.xls",skip=4,col_names=FALSE)
}
free<-read_excel("/Users/derekrouch/Documents/GitHub/data-science/free.xls",skip=4,col_names=FALSE)
read.xls("/Users/derekrouch/Documents/GitHub/data-science/free.xls",skip=4, col_names=FALSE)
read_excel("/Users/derekrouch/Documents/GitHub/data-science/free.xls")
install.packages("readxl")
free<-read_excel("/Users/derekrouch/Documents/GitHub/data-science/free.xls",skip=4,col_names=FALSE)
#library(gdata)  #gdata is finicky ... you may need to install it, but not load it.
library(tidyverse)
#library(gdata)  #gdata is finicky ... you may need to install it, but not load it.
library(tidyverse)
library(haven)
library(readxl)
free<-read_excel("/Users/derekrouch/Documents/GitHub/data-science/free.xls",skip=4,col_names=FALSE)
read.xls("/Users/derekrouch/Documents/GitHub/data-science/free.xls",skip=4, col_names=FALSE)
read_excel("/Users/derekrouch/Documents/GitHub/data-science/free.xls")
read_excel("/Users/derekrouch/Documents/GitHub/LLO8200/free.xls")
read.xls("/Users/derekrouch/Documents/GitHub/LLO8200/free.xls",skip=4, col_names=FALSE)
read_excel("/Users/derekrouch/Documents/GitHub/LLO8200/free.xls")
free<-read_excel("/Users/derekrouch/Documents/GitHub/LLO8200/free.xls",skip=4,col_names=FALSE)
free<-read_excel("/Users/derekrouch/Downloads/tabn204.10.xls",skip=4,col_names=FALSE)
View(free)
head(free)
free2<-free[ ,-(c(3,6,9,12,15,18))]
#Get rid of unwanted rows
free2<-free2%>%filter(is.na(X__1)==FALSE)
View(hsb)
head(free2)
View(free2)
#Get rid of unwanted rows
free2<-free2%>%filter(is.na(...1)==FALSE)
##50 states plus dc only
free2<-free2[2:52,]
head(free2)
tail(free2)
names(free2)<-c("state",
"total_2000",
"total_2010",
"total_2011",
"total_2012",
"frl_2000",
"frl_2010",
"frl_2011",
"frl_2012",
"pc_frl_2000",
"pc_frl_2010",
"pc_frl_2011",
"pc_frl_2012")
free_total<-free2%>%select(state,
total_2000,
total_2010,
total_2011,
total_2012)
names(free_total)<-c("state","2000","2010","2011","2012")
free_total<-free_total%>%
gather(`2000`,`2010`,`2011`,`2012`,key=year,value=total_students)
frl_total<-free2%>%select(state,
frl_2000,
frl_2010,
frl_2011,
frl_2012)
names(frl_total)<-c("state","2000","2010","2011","2012")
frl_total<-frl_total%>%gather(`2000`,`2010`,`2011`,`2012`,key=year,value=frl_students)
free_tidy<-left_join(free_total,frl_total,by=c("state","year"))
free_tidy
## Total by year
free_tidy%>%group_by(year)%>%summarize(total_students=sum(total_students),
total_frl_student=sum(frl_students))%>%
mutate(pct_frl=total_frl_student/total_students)
knitr::opts_chunk$set(echo = TRUE)
## Read in the data
gss <- read_excel("GeneralSocialSurvey1996ExcelDataStudentVersion.xls",skip=0,col_names=TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
## Read in the data
oecd <- read.delim("https://www.wiley.com/legacy/wileychi/baltagi/supp/Gasoline.dat", header = TRUE, sep="")
## Rename the columns
names(oecd) <- c("country",
"year",
"log_of_gas_consumption_per_car",
"log_of_real_per_capita_income",
"log_of_real_gas_price",
"log_of_car_stock_per_capita")
## Save as RData file
save(oecd, file = "oecd.RData")
## Read in the data
nes <- read_spss("http://www.uta.edu/faculty/story/DataSets/NES2004.sav")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
## Read in the data
oecd <- read.delim("https://www.wiley.com/legacy/wileychi/baltagi/supp/Gasoline.dat", header = TRUE, sep="")
knitr::opts_chunk$set(echo = TRUE)
## Rename the columns
names(oecd) <- c("country",
"year",
"log_of_gas_consumption_per_car",
"log_of_real_per_capita_income",
"log_of_real_gas_price",
"log_of_car_stock_per_capita")
knitr::opts_chunk$set(echo = TRUE)
## Save as RData file
save(oecd, file = "oecd.RData")
knitr::opts_chunk$set(echo = TRUE)
## Read in the data
nes <- read_spss("http://www.uta.edu/faculty/story/DataSets/NES2004.sav")
knitr::opts_chunk$set(echo = TRUE)
## Read in the data
gss <- read_excel("GeneralSocialSurvey1996ExcelDataStudentVersion.xls",skip=0,col_names=TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
## Read in the data
gss <- read_excel("GeneralSocialSurvey1996ExcelDataStudentVersion.xls",skip=0,col_names=TRUE)
knitr::opts_chunk$set(echo = TRUE)
## Read in the data
library(readxl)
knitr::opts_chunk$set(echo = TRUE)
gss <- read_excel("GeneralSocialSurvey1996ExcelDataStudentVersion.xls",skip=0,col_names=TRUE)
View(gss)
help('read_excel')
knitr::opts_chunk$set(echo = TRUE)
gss <- read_excel("GeneralSocialSurvey1996ExcelDataStudentVersion.xls",na = "",skip=0,col_names=TRUE)
View(gss)
knitr::opts_chunk$set(echo = TRUE)
gss <- read_excel("GeneralSocialSurvey1996ExcelDataStudentVersion.xls",na = "",skip=0,col_names=TRUE)
View(gss)
gss[hrs1]
View(gss)
gss[1,2]
gss[1,2]=='.'
## Clear environment
rm(list=ls())
## Get libraries
library(tidyverse)
library(nycflights13)
library(RSQLite)
## Set working director
setwd("~/Documents/GitHub/data-science")
## Establish API keys and access tokens
consumerKey='68bxc6lCLWKkMPVVKYgCzl9DO'
consumerSecret='QJumJXPJqNxTIbQm0CARNtOnOVTXGO1USqiE3OF3taMY0h99kA'
access_Token='1056575519443931136-ihgtVbXKExFBLcSo4A8gzYIEFQ9gdX'
access_Secret='xcD5nPWkFgz0586tsj4hiejHE9OoeQ1awwJLfRGCiNs4f'
requestURL='https://api.twitter.com/oauth/request_token'
accessURL='https://api.twitter.com/oauth/access_token'
authURL='https://api.twitter.com/oauth/authorize'
## Set up the OAuth credentials for a twitteR session
setup_twitter_oauth(consumerKey,consumerSecret,access_Token,access_Secret)
## Get libraries
library(twitteR)
library(ROAuth)
library(jsonlite)
library(rjson)
library(tokenizers)
library(tidyverse)
library(tm)
library(wordcloud)
## Set up the OAuth credentials for a twitteR session
setup_twitter_oauth(consumerKey,consumerSecret,access_Token,access_Secret)
## Set up the OAuth credentials for a twitteR session
setup_twitter_oauth(consumerKey,consumerSecret,access_Token,access_Secret)
## Scours twitter for hash tag
jackass <- twitteR::searchTwitter("#Trump",n=50,since="2019-09-09")
## Con
## Scours twitter for hash tag
digcit <- twitteR::searchTwitter("#DigCitCulture",n=500,since="2019-09-09")
## Convert twitteR list to data.frames
digcit_df <- twListToDF(digcit)
## Store the tweets in a .csv file
tweets_file = "tweets.csv"
(digcit_df$text[1])
## Start the file
tweets_csv <- file(tweets_file)
## Tokenize tweets into a list of words
tokens <- tokenizers::tokenize_words(digcit_df$text[1],stopwords = stopwords::stopwords("en"),
lowercase = TRUE,  strip_punct = TRUE, strip_numeric = TRUE,simplify = TRUE)
## Write squished tokens
cat(unlist(str_squish(tokens)), "\n", file=tweets_csv, sep=",")
close(tweets_csv)
## Append remaining lists of tokens into file
tweets_csv <- file(tweets_file, open = "a")
token_list = tokens
for(i in 2:nrow(digcit_df)){
tokens<-tokenize_words(digcit_df$text[i],stopwords = stopwords::stopwords("en"),
lowercase = TRUE,  strip_punct = TRUE, simplify = TRUE)
cat(unlist(str_squish(tokens)), "\n", file=tweets_csv, sep=",")
token_list <- c(token_list,  unlist(str_squish(tokens)))
}
close(tweets_csv)
## Create frequency table
cor <- Corpus(VectorSource(token_list))
tdm <- TermDocumentMatrix(cor)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
## Create a wordcloud
wordcloud(d$word,d$freq, colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 3)
## Tokenize tweets into a list of words
tokens <- tokenizers::tokenize_words(digcit_df$text[1],stopwords = stopwords::stopwords("en","t.co","https","twitter"),
lowercase = TRUE,  strip_punct = TRUE, strip_numeric = TRUE,simplify = TRUE)
## Set up the OAuth credentials for a twitteR session
setup_twitter_oauth(consumerKey,consumerSecret,access_Token,access_Secret)
## Scours twitter for hash tag
digcit <- twitteR::searchTwitter("#DigCitCulture",n=500,since="2019-09-09")
## Convert twitteR list to data.frames
digcit_df <- twListToDF(digcit)
## Store the tweets in a .csv file
tweets_file = "tweets.csv"
(digcit_df$text[1])
## Start the file
tweets_csv <- file(tweets_file)
## Tokenize tweets into a list of words
tokens <- tokenizers::tokenize_words(digcit_df$text[1],stopwords = stopwords::stopwords("en","t.co","https","twitter"),
lowercase = TRUE,  strip_punct = TRUE, strip_numeric = TRUE,simplify = TRUE)
Help(stopwords)
help(stopwords)
## Tokenize tweets into a list of words
tokens <- tokenizers::tokenize_words(digcit_df$text[1],stopwords = stopwords::stopwords("en"),
lowercase = TRUE,  strip_punct = TRUE, strip_numeric = TRUE,simplify = TRUE)
knitr::opts_chunk$set(echo = TRUE)
## Clear environment
rm(list=ls())
## Get libraries
library(twitteR)
library(ROAuth)
library(jsonlite)
library(rjson)
library(tokenizers)
library(tidyverse)
library(tm)
library(wordcloud)
## Set working director
setwd("~/Documents/GitHub/data-science")
## Establish API keys and access tokens
consumerKey='68bxc6lCLWKkMPVVKYgCzl9DO'
consumerSecret='QJumJXPJqNxTIbQm0CARNtOnOVTXGO1USqiE3OF3taMY0h99kA'
access_Token='1056575519443931136-ihgtVbXKExFBLcSo4A8gzYIEFQ9gdX'
access_Secret='xcD5nPWkFgz0586tsj4hiejHE9OoeQ1awwJLfRGCiNs4f'
requestURL='https://api.twitter.com/oauth/request_token'
accessURL='https://api.twitter.com/oauth/access_token'
authURL='https://api.twitter.com/oauth/authorize'
setup_twitter_oauth(consumerKey,consumerSecret,access_Token,access_Secret)
# Below is the function that scours twitter for a particular hash tag.
# n is the number of tweets to be collected
Search<-twitteR::searchTwitter("#DigCitCulture",n=1000,since="2019-10-03")
Search_DF <- twListToDF(Search)
# If you wish to store the tweets in a csv file ...
TransactionTweetsFile = "tweets.csv"
head(Search_DF$text[1])
## Start the file
Trans <- file(TransactionTweetsFile)
## Tokenize tweets into a list of words
Tokens<-tokenizers::tokenize_words(Search_DF$text[1],stopwords = stopwords::stopwords("en"),
lowercase = TRUE,  strip_punct = TRUE, strip_numeric = TRUE,simplify = TRUE)
## Write squished tokens
cat(unlist(str_squish(Tokens)), "\n", file=Trans, sep=",")
close(Trans)
## Append remaining lists of tokens into file
## NOTE - a list of tokens is the set of words from a Tweet
Trans <- file(TransactionTweetsFile, open = "a")
tokenList = Tokens
for(i in 2:nrow(Search_DF)){
Tokens<-tokenize_words(Search_DF$text[i],stopwords = stopwords::stopwords("en"),
lowercase = TRUE,  strip_punct = TRUE, simplify = TRUE)
cat(unlist(str_squish(Tokens)), "\n", file=Trans, sep=",")
tokenList <- c(tokenList,  unlist(str_squish(Tokens)))
}
close(Trans)
# Create a wordcloud, but first transform list of words into a
# TermDocumentMatrix
cor <- Corpus(VectorSource(tokenList))
tdm <- TermDocumentMatrix(cor)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
d_cleaned <- d[6:691,]
## NOTE:  d contains the words d$word AND frequencies d$freq
wordcloud(d_cleaned$word,d_cleaned$freq, colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 3)
View(d)
View(d)
d_cleaned <- d[8:100,]
wordcloud(d_cleaned$word,d_cleaned$freq, colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 3)
write.csv(d, file = "digcit.csv")
# Create a wordcloud, but first transform list of words into a
# TermDocumentMatrix
cor <- Corpus(VectorSource(tokenList))
tdm <- TermDocumentMatrix(cor)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
d_cleaned <- d[8:200,]
## NOTE:  d contains the words d$word AND frequencies d$freq
wordcloud(d_cleaned$word,d_cleaned$freq, colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 5)
wordcloud2(d_cleaned$word,d_cleaned$freq, colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 5)
help("wordcloud")
wordcloud(d_cleaned$word,d_cleaned$freq, colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 3)
d_cleaned <- d[8:1342,]
wordcloud(d_cleaned$word,d_cleaned$freq, colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 3)
wordcloud(d_cleaned$word,d_cleaned$freq, scale=c(4,.5), colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 3)
wordcloud(d_cleaned$word,d_cleaned$freq, scale=c(100,100), colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 3)
wordcloud(d_cleaned$word,d_cleaned$freq, scale=c(4,.1), colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 3)
wordcloud(d_cleaned$word,d_cleaned$freq, scale=c(8,.1), colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 3)
wordcloud(d_cleaned$word,d_cleaned$freq, scale=c(2,.1), colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 3)
wordcloud(d_cleaned$word,d_cleaned$freq, scale=c(2,.1), colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 5)
cor <- Corpus(VectorSource(tokenList))
tdm <- TermDocumentMatrix(cor)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
View(d)
View(d)
d_cleaned <- d[-c(2,3),]
View(d)
View(d_cleaned)
wordcloud(d_cleaned$word,d_cleaned$freq, scale=c(2,.1), colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 5)
wordcloud(d_cleaned$word,d_cleaned$freq, scale=c(4,.1), colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 5)
wordcloud(d_cleaned$word,d_cleaned$freq, scale=c(2,.4), colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 5)
wordcloud(d_cleaned$word,d_cleaned$freq, scale=c(3,.4), colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 5)
wordcloud(d_cleaned$word,d_cleaned$freq, scale=c(3,.3), colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 5)
q, scale=c(3,.3), colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min
wordcloud(d_cleaned$word,d_cleaned$freq, scale=c(3,.3), colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 5)
wordcloud(d_cleaned$word,d_cleaned$freq, scale=c(4,.3), colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 5)
wordcloud(d_cleaned$word,d_cleaned$freq, scale=c(2,.2), colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 5)
